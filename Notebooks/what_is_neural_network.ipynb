{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Neural Network?\n",
    "# It is a computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Characteristics of Artificial Neural Networks\n",
    "# A large number of very simple processing neuron-like processing elements\n",
    "\n",
    "# A large number of weighted connections between the elements\n",
    "\n",
    "# Distributed representation of knowledge over the connections\n",
    "\n",
    "# Knowledge is acquired by network through a learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of layers and number of neurons are problem dependent. we dont know how many do we need for the specific \n",
    "# problem. but we can use some references people have used before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward propagation\n",
    "- NN takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as “Forward Propagation“\n",
    "\n",
    "- Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "\n",
    "- We try to obtain the weight of neurons such that the NN total error (our cost function) being minimized. This process is known as “Backward Propagation“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to obtain best weight and biases. \n",
    "# Each perceptron has its weight and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Implementing NN using Numpy\n",
    "- Assume, we want to build and train (obtain the weights) of a MLP such that for the given input: `X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])`\n",
    "\n",
    "\n",
    "- gives us this desire output: `y=np.array([[1],[1],[0]])`\n",
    "\n",
    "\n",
    "- Also, assume we have only one hidden layer with three neurons and activation function for each perceptron is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "# output\n",
    "y = np.array([[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function(activation function)\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73105858 0.5        0.73105858 0.5       ]\n",
      " [0.73105858 0.5        0.73105858 0.73105858]\n",
      " [0.5        0.73105858 0.5        0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9794549 ]\n",
      " [0.9673252 ]\n",
      " [0.04692796]]\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "epoch=5000 #Setting training iterations\n",
    "lr = 0.1 # setting learning rate\n",
    "inputlayer_neurons = X.shape[1] # number of features \n",
    "hiddenlayer_neurons = 3 # hidden layer number of neurons\n",
    "output_neurons = 1\n",
    "\n",
    "# wieghts abd bias \n",
    "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))\n",
    "bh = np.random.uniform(size=(1, hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    # take a dot product of inputs and weight\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    # \n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "    \n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good guess at the initial steps for weights it reduces the computing time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
